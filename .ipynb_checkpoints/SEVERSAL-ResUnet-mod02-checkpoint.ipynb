{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction & Understanding The Data\n",
    "\n",
    "Whether we realize it or not, without steel our modern society would be very different! Infrastructure built with steel is resilient to heavy stress from human and nature, and therefore the production of steel is an important process. This competition asks us to help make the production process of steel more efficient by identifying defects. \n",
    "\n",
    "In this notebook I will implement a CNN architecture (ResUNet), and see if we can identify these defects! The original paper that proposes this architecture is [ResUNet-a: a deep learning framework for semantic segmentation of remotely sensed data](https://arxiv.org/abs/1904.00592).\n",
    "\n",
    "If you like my work please upvote me. Upvoting is a great postive feedback, and it helps people like me know that the work was worth it. I did this one rather quickly, so if I made a mistake, please leave a comment in the comments section to help me out. Many thanks in advance!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some basic imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# import imgaug as aug\n",
    "# import imgaug as ia\n",
    "# from imgaug import augmenters as iaa\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This competition is about instance segmentation. That is there are multiple instance of interest in the image and we need to identify each one of them. In the train file, the first column has the image id and class id seperated by a underscore. For example, in the cell below the image 0002cc93b.jpg has a fault class 1 and the mask for this fault is given. There are four fault classes and so each image appears four times in the train.csv dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.listdir('../input/severstal-steel-defect-detection/')\n",
    "from os.path import isfile, join\n",
    "\n",
    "files = [f for f in os.listdir('images/') if isfile(join('images/',f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0a9aaba9a.jpg', '0a5a82b86.jpg', '0a26aceb2.jpg', '0a8fddf7a.jpg', '0a5cff3a7.jpg', '0a3bbea4d.jpg', '0a29ef6f9.jpg', '0a5ad2f0f.jpg', '000a4bcdd.jpg', '0a4ef8ee7.jpg', '0a1cade03.jpg', '0a8cb8ddf.jpg', '0a21ce787.jpg', '0a4ad45a5.jpg', '0a9a7a6c7.jpg']\n"
     ]
    }
   ],
   "source": [
    "print(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to the train images\n",
    "# img_path = '../input/severstal-steel-defect-detection/train_images/'\n",
    "img_path = 'images/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageId_ClassId</th>\n",
       "      <th>EncodedPixels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0002cc93b.jpg_1</td>\n",
       "      <td>29102 12 29346 24 29602 24 29858 24 30114 24 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0002cc93b.jpg_2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0002cc93b.jpg_3</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0002cc93b.jpg_4</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00031f466.jpg_1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ImageId_ClassId                                      EncodedPixels\n",
       "0  0002cc93b.jpg_1  29102 12 29346 24 29602 24 29858 24 30114 24 3...\n",
       "1  0002cc93b.jpg_2                                                NaN\n",
       "2  0002cc93b.jpg_3                                                NaN\n",
       "3  0002cc93b.jpg_4                                                NaN\n",
       "4  00031f466.jpg_1                                                NaN"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('train.csv').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load full data and label no mask as -1\n",
    "\n",
    "train_df = pd.read_csv('train.csv').fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ImageId and ClassId as two seperate columns\n",
    "\n",
    "train_df['ImageId'] = train_df['ImageId_ClassId'].apply(lambda x: x.split('_')[0])\n",
    "train_df['ClassId'] = train_df['ImageId_ClassId'].apply(lambda x: x.split('_')[1])\n",
    "train_df['enhance'] = 0\n",
    "\n",
    "# dict with ClassId and EncodedPixels and group all faults by ImageId\n",
    "\n",
    "train_df['ClassId_EncodedPixels'] = train_df.apply(lambda row: (row['ClassId'], row['EncodedPixels']), axis = 1)\n",
    "grouped_EncodedPixels = train_df.groupby('ImageId')['ClassId_EncodedPixels'].apply(list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageId_ClassId</th>\n",
       "      <th>EncodedPixels</th>\n",
       "      <th>ImageId</th>\n",
       "      <th>ClassId</th>\n",
       "      <th>ClassId_EncodedPixels</th>\n",
       "      <th>enhance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0002cc93b.jpg_1</td>\n",
       "      <td>29102 12 29346 24 29602 24 29858 24 30114 24 3...</td>\n",
       "      <td>0002cc93b.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>(1, 29102 12 29346 24 29602 24 29858 24 30114 ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0002cc93b.jpg_2</td>\n",
       "      <td>-1</td>\n",
       "      <td>0002cc93b.jpg</td>\n",
       "      <td>2</td>\n",
       "      <td>(2, -1)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0002cc93b.jpg_3</td>\n",
       "      <td>-1</td>\n",
       "      <td>0002cc93b.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>(3, -1)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0002cc93b.jpg_4</td>\n",
       "      <td>-1</td>\n",
       "      <td>0002cc93b.jpg</td>\n",
       "      <td>4</td>\n",
       "      <td>(4, -1)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00031f466.jpg_1</td>\n",
       "      <td>-1</td>\n",
       "      <td>00031f466.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>(1, -1)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ImageId_ClassId                                      EncodedPixels  \\\n",
       "0  0002cc93b.jpg_1  29102 12 29346 24 29602 24 29858 24 30114 24 3...   \n",
       "1  0002cc93b.jpg_2                                                 -1   \n",
       "2  0002cc93b.jpg_3                                                 -1   \n",
       "3  0002cc93b.jpg_4                                                 -1   \n",
       "4  00031f466.jpg_1                                                 -1   \n",
       "\n",
       "         ImageId ClassId                              ClassId_EncodedPixels  \\\n",
       "0  0002cc93b.jpg       1  (1, 29102 12 29346 24 29602 24 29858 24 30114 ...   \n",
       "1  0002cc93b.jpg       2                                            (2, -1)   \n",
       "2  0002cc93b.jpg       3                                            (3, -1)   \n",
       "3  0002cc93b.jpg       4                                            (4, -1)   \n",
       "4  00031f466.jpg       1                                            (1, -1)   \n",
       "\n",
       "   enhance  \n",
       "0        0  \n",
       "1        0  \n",
       "2        0  \n",
       "3        0  \n",
       "4        0  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ClassId_EncodedPixels</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ImageId</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0002cc93b.jpg</th>\n",
       "      <td>[(1, 29102 12 29346 24 29602 24 29858 24 30114...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00031f466.jpg</th>\n",
       "      <td>[(1, -1), (2, -1), (3, -1), (4, -1)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000418bfc.jpg</th>\n",
       "      <td>[(1, -1), (2, -1), (3, -1), (4, -1)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000789191.jpg</th>\n",
       "      <td>[(1, -1), (2, -1), (3, -1), (4, -1)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0007a71bf.jpg</th>\n",
       "      <td>[(1, -1), (2, -1), (3, 18661 28 18863 82 19091...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           ClassId_EncodedPixels\n",
       "ImageId                                                         \n",
       "0002cc93b.jpg  [(1, 29102 12 29346 24 29602 24 29858 24 30114...\n",
       "00031f466.jpg               [(1, -1), (2, -1), (3, -1), (4, -1)]\n",
       "000418bfc.jpg               [(1, -1), (2, -1), (3, -1), (4, -1)]\n",
       "000789191.jpg               [(1, -1), (2, -1), (3, -1), (4, -1)]\n",
       "0007a71bf.jpg  [(1, -1), (2, -1), (3, 18661 28 18863 82 19091..."
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_grouped_EncodedPixels = pd.DataFrame(grouped_EncodedPixels)\n",
    "df_grouped_EncodedPixels.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data distribution\n",
    "The dataset has lots of non error images while errors are 14,2 % of the images, and within the 7095 errors, we have a big amount of class 2 errors (72,5 %)\n",
    "The data sample is clearly umbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50272it [00:03, 12822.27it/s]\n",
      "1254it [00:00, 12539.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First iter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12568it [00:00, 13277.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Second iter\n",
      "Num of elements by class:[43177, 897, 247, 5150, 801]         \n",
      "Num of elements: 50272         \n",
      "Num defaults: 7095         \n",
      "Num of images: 12568         \n",
      "train_df length: 50272         \n",
      "Defaults by Class Id: [5902, 6239, 425, 2, 0]         \n",
      "Unique defaults by class: [769, 195, 4759, 516]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# for each fault class, here the mumber of positive incidences\n",
    "count_classId = [0,0,0,0,0]\n",
    "for i, row in tqdm(train_df.iterrows()):\n",
    "    if row['EncodedPixels'] != -1:\n",
    "        count_classId[int(row['ClassId'])] += 1\n",
    "    else:\n",
    "        count_classId[0] += 1\n",
    "\n",
    "print('\\nFirst iter')\n",
    "\n",
    "# represents number of samples with 0, 1, 2, 3 or 4 fault classes simultaneously\n",
    "defaults_by_classId = [0,0,0,0,0]\n",
    "unique_defaults = [0,0,0,0]\n",
    "unique_default_id = {}\n",
    "for i, row in tqdm(df_grouped_EncodedPixels.iterrows()):\n",
    "    n = 0\n",
    "    for xx in row:\n",
    "        for x in xx:\n",
    "            if x[1] != -1:\n",
    "                n += 1\n",
    "                v = int(x[0])\n",
    "        if n == 1:\n",
    "            unique_defaults[v-1] += 1\n",
    "            unique_default_id[i] = v\n",
    "            \n",
    "    defaults_by_classId[n] +=1\n",
    "    \n",
    "print('\\nSecond iter')        \n",
    "num_defaults = sum(count_classId) - count_classId[0]\n",
    "\n",
    "print('Num of elements by class:{} \\\n",
    "        \\nNum of elements: {} \\\n",
    "        \\nNum defaults: {} \\\n",
    "        \\nNum of images: {} \\\n",
    "        \\ntrain_df length: {} \\\n",
    "        \\nDefaults by Class Id: {} \\\n",
    "        \\nUnique defaults by class: {}'.format(count_classId, \n",
    "                                    sum(count_classId),\n",
    "                                    num_defaults,\n",
    "                                    len(grouped_EncodedPixels),\n",
    "                                    len(train_df),\n",
    "                                    defaults_by_classId,\n",
    "                                    unique_defaults))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6239\n"
     ]
    }
   ],
   "source": [
    "print(len(unique_default_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll create a new dataset reusing the undersampled classes so to have a complete representation of all of them.\n",
    "we'll apply some random data augmentation, applying shifts up and down, resizing, and rolling with different values. \n",
    "With this we'll be able to obtain someting like 10x number of samples for the most underepresented ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create deep copy of train_df dataframe where I'll increase cases\n",
    "# in order to homogeinize the training set.\n",
    "train_df_enh = train_df.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in unique_default_id.keys():\n",
    "    num_enh = random.randit(1,5)\n",
    "    for n in range(num_enh):\n",
    "        # have to include new lines with some kind of modification of the \n",
    "        # image and mask\n",
    "        train_df_enh.append()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# picking up 10 examples with at least two faults for visualization\n",
    "\n",
    "samples = []\n",
    "for f in files:\n",
    "    r = grouped_EncodedPixels[f]\n",
    "    if (len([x[1] for x in r if x[1] != -1]) >= 1) and (len(samples) < 10):\n",
    "        samples.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Masks are obviously encoded. we use the following function to \n",
    "# decode the masks. I picked this function up from a Kaggle kernel, \n",
    "# so thanks to the authors of https://www.kaggle.com/robertkag/rle-to-mask-converter\n",
    "\n",
    "def rleToMask(rleString, height, width):\n",
    "    rows, cols = height, width\n",
    "    rleNumbers = [int(numstring) for numstring in rleString.split(' ')]\n",
    "    rlePairs = np.array(rleNumbers).reshape(-1,2)\n",
    "    img = np.zeros(rows * cols, dtype=np.uint8)\n",
    "    for index, length in rlePairs:\n",
    "        index -= 1\n",
    "        img[index : index+length] = 255\n",
    "    img = img.reshape(cols,rows)\n",
    "    img = img.T\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show steel image with four classes of faults in seperate columns\n",
    "def show_image_masks(img, masks, convert_to_float=False):\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=4, sharey=True, figsize=(20,10))\n",
    "    cmaps = [\"Reds\", \"Blues\", \"Greens\", \"Purples\"]\n",
    "    for i, mask in enumerate(masks):\n",
    "        if mask == -1:\n",
    "            mask_decoded = np.zeros((256, 1600))\n",
    "        else:\n",
    "            mask_decoded = rleToMask(mask, 256, 1600)\n",
    "        ax[i].imshow(img)\n",
    "        ax[i].imshow(mask_decoded, alpha=0.5, cmap=cmaps[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the image we picked up earlier with mask\n",
    "for sample in samples:\n",
    "    mask_1, mask_2, mask_3, mask_4 = grouped_EncodedPixels[sample]\n",
    "    masks = [mask_1[1], mask_2[1], mask_3[1], mask_4[1]]\n",
    "    img = cv2.imread(os.path.join(img_path, sample))\n",
    "    show_image_masks(img, masks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Configurations\n",
    "\n",
    "As mentioned earlier, this notebook I will implement a ResUNet to predict the different classes of faults. As the image we have is colorful, we will have 3 channels in our input layer. As we have four types of masks, we will have four channels in output layer to pick up different categories of faults."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network configuration parameters\n",
    "# original image is 1600x256, so we will resize it to 1/4 (400,64) to test model\n",
    "\n",
    "img_w = 400\n",
    "img_h = 64\n",
    "img_size = (img_w, img_h)\n",
    "\n",
    "# batch size for training unet\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "# kernel size 3x3\n",
    "\n",
    "k_size = 3 \n",
    "\n",
    "# split of training and validation set\n",
    "\n",
    "val_size = .20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for building the network\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import reduce_sum\n",
    "from tensorflow.keras.backend import pow\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPool2D, UpSampling2D, Concatenate, Add, Flatten\n",
    "from tensorflow.keras.losses import binary_crossentropy\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augmentation\n",
    "## Forked from: https://github.com/aleju/imgaug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sometimes = lambda aug: iaa.Sometimes(0.5, aug)\n",
    "seq = iaa.Sequential(\n",
    "        [\n",
    "            # apply the following augmenters to most images\n",
    "            iaa.Fliplr(0.5), # horizontally flip 50% of all images\n",
    "            iaa.Flipud(0.2), # vertically flip 20% of all images\n",
    "            sometimes(iaa.Affine(\n",
    "                # scale images to 90-110% of their size, individually per axis\n",
    "                scale={\"x\": (0.9, 1.1), \"y\": (0.9, 1.1)}, \n",
    "                # traslate by -10 to +10 percent\n",
    "                translate_percent={\"x\": (-0.1, 0.1), \"y\": (-0.1, 0.1)}, \n",
    "                # use nearest neighbour or bilinear interpolation (fast)\n",
    "                order=[0, 1], \n",
    "                # if mode is constant, use a cval between 0 and 255\n",
    "                cval=(0, 255), \n",
    "                # use any of scikit-image's warping modes\n",
    "                mode=ia.ALL\n",
    "            ))\n",
    "        ],\n",
    "        random_order=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generator\n",
    "\n",
    "To push the data to our model, we will create a custom data generator. A generator lets us load data progressively, instead of loading it all into memory at once. A custom generator allows us to also fit in more customization during the time of loading the data. As the model is being procssed in the GPU, we can use a custom generator to pre-process images via a generator. At this time, we can also take advantage multiple processors to parallelize our pre-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new DataGenerator class\n",
    "# improved with a train and a test position \n",
    "# uses image transform function to process te image\n",
    "\n",
    "class DataGenerator(Sequence):\n",
    "\n",
    "    def __init__(self, \n",
    "                 image_filenames, \n",
    "                 labels,\n",
    "                 image_size = (400,64),\n",
    "                 batch_size=16, \n",
    "                 is_train=True,\n",
    "                 mix=False, \n",
    "                 augment=False):\n",
    "        \n",
    "        self.image_filenames, self.labels = image_filenames, labels\n",
    "        self.image_size = image_size\n",
    "        self.batch_size = batch_size\n",
    "        self.is_train = is_train\n",
    "        self.is_augment = augment\n",
    "        if(self.is_train):\n",
    "            self.on_epoch_end()\n",
    "        self.is_mix = mix\n",
    "\n",
    "    def __len__(self):\n",
    "        # number of batches per epoch\n",
    "        return int(np.ceil(len(self.image_filenames) / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # generate one batch of data\n",
    "        batch_x = self.image_filenames[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_y = self.labels[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "\n",
    "        if(self.is_train):\n",
    "            return self.train_generate(batch_x, batch_y)\n",
    "        return self.valid_generate(batch_x, batch_y)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if(self.is_train):\n",
    "            self.image_filenames, self.labels = shuffle(self.image_filenames, self.labels)\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    def image_transform(self, image):\n",
    "        resized = cv2.resize(image, self.img_size , interpolation=cv2.INTER_CUBIC)\n",
    "        return resized\n",
    "\n",
    "    def mix_up(self, x, y):\n",
    "        lam = np.random.beta(0.2, 0.4)\n",
    "        ori_index = np.arange(int(len(x)))\n",
    "        index_array = np.arange(int(len(x)))\n",
    "        np.random.shuffle(index_array)        \n",
    "        \n",
    "        mixed_x = lam * x[ori_index] + (1 - lam) * x[index_array]\n",
    "        mixed_y = lam * y[ori_index] + (1 - lam) * y[index_array]\n",
    "        \n",
    "        return mixed_x, mixed_y\n",
    "\n",
    "    def train_generate(self, batch_x, batch_y):\n",
    "        batch_images = []\n",
    "        for (i, label) in zip(batch_x, batch_y):\n",
    "            # file=f\"/mnt/DATA-SSD/DataSandbox/APTOS/train_images/{i}.png\"\n",
    "            file = f\"images/{i}\"\n",
    "            img = cv2.imread(file)\n",
    "            # any image modification needed\n",
    "            img = img_transform(img,2, size)\n",
    "            if(self.is_augment):\n",
    "                img = seq.augment_image(img)\n",
    "            batch_images.append(img)\n",
    "            \n",
    "        batch_images = np.array(batch_images, np.float32) / 255\n",
    "        batch_y = np.array(batch_y, np.float32)\n",
    "        if(self.is_mix):\n",
    "            batch_images, batch_y = self.mix_up(batch_images, batch_y)\n",
    "        return batch_images, batch_y\n",
    "\n",
    "    def valid_generate(self, batch_x, batch_y):\n",
    "        batch_images = []\n",
    "        for (i, label) in zip(batch_x, batch_y):\n",
    "            # file=f\"/mnt/DATA-SSD/DataSandbox/APTOS/train_images/{i}.png\"\n",
    "            file = f\"images/{i}\"\n",
    "            img = cv2.imread(file)\n",
    "            # any image modification needed\n",
    "            img = img_transform(img,2,size)\n",
    "            batch_images.append(img)\n",
    "        batch_images = np.array(batch_images, np.float32) / 255\n",
    "        batch_y = np.array(batch_y, np.float32)\n",
    "        return batch_images, batch_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dict of all the masks\n",
    "masks = {}\n",
    "for index, row in train_df[train_df['EncodedPixels']!=-1].iterrows():\n",
    "    masks[row['ImageId_ClassId']] = row['EncodedPixels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the training data into train and validation set (stratified)\n",
    "X_train, X_val = train_test_split(train_df[train_df['EncodedPixels']!=-1]['ImageId'].unique(), test_size=val_size, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'img_h': img_h,\n",
    "          'img_w': img_w,\n",
    "          'batch_size': batch_size,\n",
    "          'shuffle': True}\n",
    "\n",
    "# Get Generators\n",
    "training_generator = DataGenerator(X_train, masks, **params)\n",
    "validation_generator = DataGenerator(X_val, masks, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check out the shapes\n",
    "x, y = training_generator.__getitem__(0)\n",
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets visualize some images with their faults to make sure our data generator is working like it should\n",
    "for ix in range(0,batch_size):\n",
    "    if y[ix].sum() > 0:\n",
    "        img = x[ix]\n",
    "        masks_temp = [y[ix][...,i] for i in range(0,4)]\n",
    "        show_image_masks(img, masks_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resunet\n",
    "\n",
    "In this section we will define the building blocks for our network and train our network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bn_act(x, act=True):\n",
    "    # batch normalization layer with a relu activation layer\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    if act == True:\n",
    "        x = tf.keras.layers.Activation('relu')(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(x, filters, kernel_size=3, padding='same', strides=1):\n",
    "    # convolutional layer which always uses the batch normalization layer\n",
    "    conv = bn_act(x)\n",
    "    conv = Conv2D(filters, kernel_size, padding=padding, strides=strides)(conv)\n",
    "    return conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(x, filters, kernel_size=3, padding='same', strides=1):\n",
    "    conv = Conv2D(filters, kernel_size, padding=padding, strides=strides)(x)\n",
    "    conv = conv_block(conv, filters, kernel_size, padding, strides)\n",
    "    shortcut = Conv2D(filters, kernel_size=1, padding=padding, strides=strides)(x)\n",
    "    shortcut = bn_act(shortcut, act=False)\n",
    "    output = Add()([conv, shortcut])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_block(x, filters, kernel_size=3, padding='same', strides=1):\n",
    "    res = conv_block(x, filters, k_size, padding, strides)\n",
    "    res = conv_block(res, filters, k_size, padding, 1)\n",
    "    shortcut = Conv2D(filters, kernel_size, padding=padding, strides=strides)(x)\n",
    "    shortcut = bn_act(shortcut, act=False)\n",
    "    output = Add()([shortcut, res])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsample_concat_block(x, xskip):\n",
    "    u = UpSampling2D((2,2))(x)\n",
    "    c = Concatenate()([u, xskip])\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResUNet(img_h, img_w):\n",
    "    f = [16, 32, 64, 128, 256]\n",
    "    inputs = Input((img_h, img_w, 3))\n",
    "    \n",
    "    ## Encoder\n",
    "    e0 = inputs\n",
    "    e1 = stem(e0, f[0])\n",
    "    e2 = residual_block(e1, f[1], strides=2)\n",
    "    e3 = residual_block(e2, f[2], strides=2)\n",
    "    e4 = residual_block(e3, f[3], strides=2)\n",
    "    e5 = residual_block(e4, f[4], strides=2)\n",
    "    \n",
    "    ## Bridge\n",
    "    b0 = conv_block(e5, f[4], strides=1)\n",
    "    b1 = conv_block(b0, f[4], strides=1)\n",
    "    \n",
    "    ## Decoder\n",
    "    u1 = upsample_concat_block(b1, e4)\n",
    "    d1 = residual_block(u1, f[4])\n",
    "    \n",
    "    u2 = upsample_concat_block(d1, e3)\n",
    "    d2 = residual_block(u2, f[3])\n",
    "    \n",
    "    u3 = upsample_concat_block(d2, e2)\n",
    "    d3 = residual_block(u3, f[2])\n",
    "    \n",
    "    u4 = upsample_concat_block(d3, e1)\n",
    "    d4 = residual_block(u4, f[1])\n",
    "    \n",
    "    outputs = tf.keras.layers.Conv2D(4, (1, 1), padding=\"same\", activation=\"sigmoid\")(d4)\n",
    "    model = tf.keras.models.Model(inputs, outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Functions\n",
    "\n",
    "As our classes are highly imbalanced and we are stacking four output layers at once, it is even more important to get the loss function right. Here I will aggregate important loss functions so you can reuse and experiment with them along with me."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dice similarity coefficient loss\n",
    "# https://github.com/nabsabraham/focal-tversky-unet\n",
    "\n",
    "def dsc(y_true, y_pred):\n",
    "    smooth = 1.\n",
    "    y_true_f = Flatten()(y_true)\n",
    "    y_pred_f = Flatten()(y_pred)\n",
    "    intersection = reduce_sum(y_true_f * y_pred_f)\n",
    "    score = (2. * intersection + smooth) / (reduce_sum(y_true_f) + reduce_sum(y_pred_f) + smooth)\n",
    "    return score\n",
    "\n",
    "def dice_loss(y_true, y_pred):\n",
    "    loss = 1 - dsc(y_true, y_pred)\n",
    "    return loss\n",
    "\n",
    "def bce_dice_loss(y_true, y_pred):\n",
    "    loss = binary_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Focal Tversky loss\n",
    "# https://github.com/nabsabraham/focal-tversky-unet\n",
    "\n",
    "def tversky(y_true, y_pred, smooth=1e-6):\n",
    "    y_true_pos = tf.keras.layers.Flatten()(y_true)\n",
    "    y_pred_pos = tf.keras.layers.Flatten()(y_pred)\n",
    "    true_pos = tf.reduce_sum(y_true_pos * y_pred_pos)\n",
    "    false_neg = tf.reduce_sum(y_true_pos * (1-y_pred_pos))\n",
    "    false_pos = tf.reduce_sum((1-y_true_pos)*y_pred_pos)\n",
    "    alpha = 0.7\n",
    "    return (true_pos + smooth)/(true_pos + alpha*false_neg + (1-alpha)*false_pos + smooth)\n",
    "\n",
    "def tversky_loss(y_true, y_pred):\n",
    "    return 1 - tversky(y_true,y_pred)\n",
    "\n",
    "def focal_tversky_loss(y_true,y_pred):\n",
    "    pt_1 = tversky(y_true, y_pred)\n",
    "    gamma = 0.75\n",
    "    return tf.keras.backend.pow((1-pt_1), gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile & Fit The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResUNet(img_h=img_h, img_w=img_w)\n",
    "adam = tf.keras.optimizers.Adam(lr = 0.05, epsilon = 0.1)\n",
    "model.compile(optimizer=adam, loss=focal_tversky_loss, metrics=[dsc, tversky])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    model.load_weights('../input/severstal-data/ResUNetSteelFaultOnly.h5')\n",
    "except OSError:\n",
    "    print('You need to run the model and load the trained model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit_generator(generator=training_generator, validation_data=validation_generator, epochs=5, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./ResUNetSteelFaultOnly.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Insights\n",
    "\n",
    "In this section we take a look at the performance of our model and visually inspect how our predictions look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.figure(figsize=(20,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(history.history['tversky'])\n",
    "plt.plot(history.history['val_tversky'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "\n",
    "# summarize history for loss\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to plot image with mask and image with predicted mask next to each other\n",
    "def viz_single_fault(img, mask, pred, image_class):\n",
    "    \n",
    "    fig, ax = plt.subplots(nrows=1, ncols=2, sharey=True, figsize=(15,5))\n",
    "    \n",
    "    cmaps = [\"Reds\", \"Blues\", \"Greens\", \"Purples\"]\n",
    "    \n",
    "    ax[0].imshow(img)\n",
    "    ax[0].imshow(mask, alpha=0.3, cmap=cmaps[image_class-1])\n",
    "    ax[0].set_title('Mask - Defect Class %s' % image_class)\n",
    "    \n",
    "    ax[1].imshow(img)\n",
    "    ax[1].imshow(pred, alpha=0.3, cmap=cmaps[image_class-1])\n",
    "    ax[1].set_title('Predicted Mask - Defect Class %s' % image_class)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a better understanding of our model we will calculate the IoU score. If you are unfamiliar with the concept of IoU, you can read more of it here: http://ronny.rest/tutorials/module/localization_001/iou/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.jeremyjordan.me/evaluating-image-segmentation-models/\n",
    "def calculate_iou(target, prediction):\n",
    "    intersection = np.logical_and(target, prediction)\n",
    "    union = np.logical_or(target, prediction)\n",
    "    if np.sum(union) == 0:\n",
    "        iou_score = 0\n",
    "    else:\n",
    "        iou_score = np.sum(intersection) / np.sum(union)\n",
    "    return iou_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets loop over the predictions and print 5 of each image cases with defects\n",
    "count = 0\n",
    "# a list to keep count of the number of plots made per image class\n",
    "class_viz_count = [0,0,0,0]\n",
    "# to keep the total iou score per image class\n",
    "class_iou_score = [0, 0, 0, 0]\n",
    "# to keep sum of mask pixels per image class\n",
    "class_mask_sum = [0, 0, 0, 0]\n",
    "# to keep sum of predicted mask pixels per image class\n",
    "class_pred_sum = [0, 0, 0, 0]\n",
    "\n",
    "# loop over to all the batches in one epoch \n",
    "for i in range(0, validation_generator.__len__()):\n",
    "    # get a batch of image, true mask, and predicted mask\n",
    "    x, y = validation_generator.__getitem__(i)\n",
    "    predictions = model.predict(x)\n",
    "    \n",
    "    # loop through x to get all the images in the batch\n",
    "    for idx, val in enumerate(x):\n",
    "        # we are only interested if there is a fault. if we are dropping images with no faults before this will become redundant\n",
    "        if y[idx].sum() > 0: \n",
    "            # get an image and convert to make it matplotlib.pyplot friendly\n",
    "            img = x[idx]\n",
    "            img = cv2.cvtColor(img.astype('float32'), cv2.COLOR_BGR2RGB)\n",
    "            # loop over the four ourput layers to create a list of all the masks for this image\n",
    "            masks_temp = [y[idx][...,i] for i in range(0,4)]\n",
    "            # loop over the four output layers to create a list of all the predictions for this image\n",
    "            preds_temp = [predictions[idx][...,i] for i in range(0,4)]\n",
    "            # turn to binary (prediction) mask \n",
    "            preds_temp = [p > .5 for p in preds_temp]\n",
    "            \n",
    "            for i, (mask, pred) in enumerate(zip(masks_temp, preds_temp)):\n",
    "                image_class = i + 1\n",
    "                class_iou_score[i] += calculate_iou(mask, pred)\n",
    "                class_mask_sum[i] += mask.sum()\n",
    "                class_pred_sum[i] += pred.sum()\n",
    "                if mask.sum() > 0 and class_viz_count[i] < 5:\n",
    "                    viz_single_fault(img, mask, pred, image_class)\n",
    "                    class_viz_count[i] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the IoU, Sum of pixel for mask given, and sum of pixel for the prediction.\n",
    "class_ids = [1,2,3,4]\n",
    "plt.figure(figsize=(20,5))\n",
    "plt.subplot(1,3,1)\n",
    "y_pos = np.arange(len(class_ids))\n",
    "plt.bar(y_pos, class_iou_score)\n",
    "plt.xticks(y_pos, class_ids)\n",
    "plt.title('IoU score per class')\n",
    "plt.ylabel('IoU Sum')\n",
    "plt.xlabel('class id')\n",
    "plt.subplot(1,3,2)\n",
    "plt.bar(y_pos, class_mask_sum)\n",
    "plt.xticks(y_pos, class_ids)\n",
    "plt.title('labeled mask pixel sum per class')\n",
    "plt.ylabel('pixel sum')\n",
    "plt.xlabel('class id')\n",
    "plt.ticklabel_format(axis='y',style='sci',scilimits=(1,4))\n",
    "plt.subplot(1,3,3)\n",
    "plt.bar(y_pos, class_pred_sum)\n",
    "plt.xticks(y_pos, class_ids)\n",
    "plt.title('predicted mask pixel sum per class')\n",
    "plt.ylabel(' pixel sum')\n",
    "plt.xlabel('class id')\n",
    "plt.ticklabel_format(axis='y',style='sci',scilimits=(1,4))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Its obviously and unfortunately clear that we don't see enough sample of the other classes so we don't make a any prediction on them. I do wish we made some prediciton on class 4 but oh well, life building AI isn't easy =( "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Predictions & Submission File\n",
    "\n",
    "Now that our model is trained lets make a submission and file it in!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return tensor in the right shape for prediction \n",
    "def get_test_tensor(img_dir, img_h, img_w, channels=3):\n",
    "\n",
    "    X = np.empty((1, img_h, img_w, channels))\n",
    "\n",
    "    # Store sample\n",
    "    image = cv2.imread(img_dir)\n",
    "    image_resized = cv2.resize(image, (img_w, img_h))\n",
    "    image_resized = np.array(image_resized, dtype=np.float64)\n",
    "    X[0,] = image_resized\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is an awesome little function to remove small spots in our predictions\n",
    "\n",
    "from skimage import morphology\n",
    "\n",
    "def remove_small_regions(img, size):\n",
    "    \"\"\"Morphologically removes small (less than size) connected regions of 0s or 1s.\"\"\"\n",
    "    img = morphology.remove_small_objects(img, size)\n",
    "    img = morphology.remove_small_holes(img, size)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "# get all files using glob\n",
    "test_files = [f for f in glob.glob('../input/severstal-steel-defect-detection/test_images/' + \"*.jpg\", recursive=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will use this function to convert our mask into RLE. Thanks to the authors of: https://www.kaggle.com/paulorzp/rle-functions-run-lenght-encode-decode\n",
    "def mask2rle(img):\n",
    "    '''\n",
    "    img: numpy array, 1 - mask, 0 - background\n",
    "    Returns run length as string formated\n",
    "    '''\n",
    "    pixels= img.T.flatten()\n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    return ' '.join(str(x) for x in runs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = []\n",
    "\n",
    "# a function to apply all the processing steps necessery to each of the individual masks\n",
    "def process_pred_mask(pred_mask):\n",
    "    \n",
    "    pred_mask = cv2.resize(pred_mask.astype('float32'),(1600, 256))\n",
    "    pred_mask = (pred_mask > .5).astype(int)\n",
    "    pred_mask = remove_small_regions(pred_mask, 0.02 * np.prod(512)) * 255\n",
    "    pred_mask = mask2rle(pred_mask)\n",
    "    \n",
    "    #if len(pred_mask) < 1:\n",
    "    #    pred_mask = ''\n",
    "    \n",
    "    return pred_mask\n",
    "\n",
    "# loop over all the test images\n",
    "for f in test_files:\n",
    "    # get test tensor, output is in shape: (1, 256, 512, 3)\n",
    "    test = get_test_tensor(f, img_h, img_w) \n",
    "    # get prediction, output is in shape: (1, 256, 512, 4)\n",
    "    pred_masks = model.predict(test) \n",
    "    # get a list of masks with shape: 256, 512\n",
    "    pred_masks = [pred_masks[0][...,i] for i in range(0,4)]\n",
    "    # apply all the processing steps to each of the mask\n",
    "    pred_masks = [process_pred_mask(pred_mask) for pred_mask in pred_masks]\n",
    "    # get our image id\n",
    "    id = f.split('/')[-1]\n",
    "    # create ImageId_ClassId and get the EncodedPixels for the class ID, and append to our submissions list\n",
    "    [submission.append((id+'_%s' % (k+1), pred_mask)) for k, pred_mask in enumerate(pred_masks)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to a csv\n",
    "submission_df = pd.DataFrame(submission, columns=['ImageId_ClassId', 'EncodedPixels'])\n",
    "# check out some predictions and see if it looks good\n",
    "submission_df[ submission_df['EncodedPixels'] != '1 1'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write it out\n",
    "submission_df.to_csv('./submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Section\n",
    "\n",
    "1. For this competition, as it is a Kernel only competition, your maximum Kernel runtime is 1 hour. I forgot that like a dummy and in my first attempt, this version of the [kernel](https://www.kaggle.com/ekhtiar/eda-resunet-tensorflow-keras?scriptVersionId=18015343), I didn't manage to make a submission. In the next iteration, I will save the model and re-run it.\n",
    "2. After pre-training our model for 50 epoch, I managed to load that and make a submission in this version of the [kernel](https://www.kaggle.com/ekhtiar/eda-resunet-tensorflow-keras?scriptVersionId=18052749). In this kernel we just had 5 additional epochs on top of the pre-trained model and the score was 0.71469.\n",
    "3. Its very obvious that for Image class 3 we are doing a very good job. For the rest of the classes, we are not even being triggered. This is due to the imbalance in our classes. I tried dropping all the images with no masks at all, and retraining the model in this [kernel](https://www.kaggle.com/ekhtiar/resunet-baseline-with-tensorflow?scriptVersionId=18077319). However, it didn't help!\n",
    "4. From all the kernels, we can observe that 20 epoch is good enough. So I will reduce my epochs to 20 from now on.\n",
    "5. The loss function was changed to focal tversky loss, in this [kernel](https://www.kaggle.com/ekhtiar/resunet-baseline-on-tensorflow-w-tutorial-notes?scriptVersionId=18088542). We still didn't improve our predictions for other classes. One thing for sure, we need to zoom into the performance of the model more. And therefore section \"Model Insights\" is created/extended.\n",
    "6. In future I will take step to balance the data more for each of the classes during the training process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay guys! This is just a start and hopefully a good start. I will be back soon with update to make this much more professional. \n",
    "\n",
    "If you like my work please upvote me. Upvoting is a great postive feedback, and it helps people like me know that the work was worth it. I did this one rather quickly, so if I made a mistake, please leave a comment in the comments section to help me out. Many thanks in advance!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
